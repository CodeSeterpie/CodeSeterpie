{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputファイルを表示: 2020-02-14 06:22:45.260001\n",
      "/kaggle/input/test.csv\n",
      "/kaggle/input/data_description.txt\n",
      "/kaggle/input/train.csv\n",
      "/kaggle/input/.ipynb_checkpoints/train-checkpoint.csv\n",
      "/kaggle/input/.ipynb_checkpoints/test-checkpoint.csv\n",
      "/kaggle/input/.ipynb_checkpoints/data_description-checkpoint.txt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import itertools\n",
    "\n",
    "# ログ出力用の関数\n",
    "def printTime(tag):\n",
    "    print(tag + ':', datetime.datetime.now())\n",
    "\n",
    "printTime('Inputファイルを表示')\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学習データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/test.csv')\n",
    "\n",
    "# 学習データを特徴量と目的変数に分ける\n",
    "train_x = train.drop(['SalePrice'], axis=1)\n",
    "train_y = train['SalePrice']\n",
    "\n",
    "# テストデータは特徴量のみなので、そのままでよい\n",
    "test_x = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 特徴量作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解析対象とする数値列を設定\n",
    "targets_num = ['1stFlrSF', \n",
    "               '2ndFlrSF'\n",
    "               'BsmtFinSF1', \n",
    "               'Fireplaces', \n",
    "               'FullBath', \n",
    "               'GarageArea', \n",
    "               'GarageCars', \n",
    "               'GarageYrBlt', \n",
    "               'GrLivArea', \n",
    "               'MasVnrArea', \n",
    "               'OverallQual', \n",
    "               'OverallCond',\n",
    "               'TotalBsmtSF', \n",
    "               'TotRmsAbvGrd', \n",
    "               'YearBuilt', \n",
    "               'YearRemodAdd']\n",
    "\n",
    "# 解析対象とするカテゴリ列を設定\n",
    "targets_category = [\n",
    "                    'MSSubClass',\n",
    "                    'MSZoning',\n",
    "                    'Street',\n",
    "                    'Alley',\n",
    "                    'LotShape',\n",
    "                    'LandContour',\n",
    "                    'Utilities',\n",
    "                    'LotConfig',\n",
    "                    'LandSlope',\n",
    "                    'Neighborhood',\n",
    "                    'Condition1',\n",
    "                    'Condition2',\n",
    "                    'BldgType',\n",
    "                    'HouseStyle',\n",
    "                    'RoofStyle',\n",
    "                    'RoofMatl',\n",
    "                    'Exterior1st',\n",
    "                    'Exterior2nd',\n",
    "                    'MasVnrType',\n",
    "                    'ExterQual',\n",
    "                    'ExterCond',\n",
    "                    'Foundation',\n",
    "                    'BsmtQual',\n",
    "                    'BsmtCond',\n",
    "                    'BsmtExposure',\n",
    "                    'BsmtFinType1',\n",
    "                    'BsmtFinType2',\n",
    "                    'Heating',\n",
    "                    'HeatingQC',\n",
    "                    'CentralAir',\n",
    "                    'Electrical',\n",
    "                    'KitchenQual',\n",
    "                    'Functional',\n",
    "                    'FireplaceQu',\n",
    "                    'GarageType',\n",
    "                    'GarageFinish',\n",
    "                    'GarageQual',\n",
    "                    'GarageCond',\n",
    "                    'PavedDrive',\n",
    "                    'PoolQC',\n",
    "                    'Fence',\n",
    "                    'MiscFeature',\n",
    "                    'SaleType',\n",
    "                    'SaleCondition']\n",
    "\n",
    "# 解析対象とする数値列とカテゴリ列の列名を連結\n",
    "targets = targets_num\n",
    "targets.extend(targets_category)\n",
    "\n",
    "# 解析対象列のみを抽出\n",
    "train_x = train_x.reindex(columns=targets)\n",
    "test_x = test_x.reindex(columns=targets)\n",
    "\n",
    "# Dataframeの列の型がobjectのままだとLabelEncoderでエラーするため、strに変換\n",
    "# ※変換処理後にinfo()で型を確認しても、objectと表示されるが内部的にはstrに変換されている。\n",
    "train_x[targets_category] = train_x[targets_category].astype(str)\n",
    "test_x[targets_category] = test_x[targets_category].astype(str)\n",
    "\n",
    "# ラベルエンコーディング用に学習データとテストデータをunion\n",
    "lavel_x = train_x.append(test_x, ignore_index=True)\n",
    "\n",
    "# カテゴリ変数をlabel encodingする\n",
    "for c in targets_category:\n",
    "    # 学習データとテストデータに基づいてラベルを設定\n",
    "    le = LabelEncoder()\n",
    "    le.fit(lavel_x[c])\n",
    "    train_x[c] = le.transform(train_x[c])\n",
    "    test_x[c] = le.transform(test_x[c])\n",
    "\n",
    "    \n",
    "# 数値変数のnanを0に変換する\n",
    "for c in targets_num:\n",
    "    train_x[c] = train_x[c].fillna(0)\n",
    "    test_x[c] = test_x[c].fillna(0)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### モデル作成とバリデーション\n",
    "XGBoostを使用してモデルを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデルの作成開始: 2020-02-14 06:28:02.883738\n",
      "Train on 1095 samples, validate on 365 samples\n",
      "Epoch 1/150\n",
      "1095/1095 [==============================] - 1s 494us/sample - loss: 39288069558.5900 - val_loss: 38262174717.1945\n",
      "Epoch 2/150\n",
      "1095/1095 [==============================] - 0s 97us/sample - loss: 39251136242.6740 - val_loss: 38187073266.6740\n",
      "Epoch 3/150\n",
      "1095/1095 [==============================] - 0s 104us/sample - loss: 39112151447.7297 - val_loss: 37953309463.1452\n",
      "Epoch 4/150\n",
      "1095/1095 [==============================] - 0s 95us/sample - loss: 38722610573.4429 - val_loss: 37357951419.2658\n",
      "Epoch 5/150\n",
      "1095/1095 [==============================] - 0s 114us/sample - loss: 37774704722.2941 - val_loss: 36045719041.4027\n",
      "Epoch 6/150\n",
      "1095/1095 [==============================] - 0s 105us/sample - loss: 35801056384.1169 - val_loss: 33529349004.9753\n",
      "Epoch 7/150\n",
      "1095/1095 [==============================] - 0s 113us/sample - loss: 32273044058.2429 - val_loss: 29266159318.6192\n",
      "Epoch 8/150\n",
      "1095/1095 [==============================] - 0s 112us/sample - loss: 26710781334.7945 - val_loss: 23066563485.8082\n",
      "Epoch 9/150\n",
      "1095/1095 [==============================] - 0s 112us/sample - loss: 19343899653.6110 - val_loss: 15818726820.8219\n",
      "Epoch 10/150\n",
      "1095/1095 [==============================] - 0s 101us/sample - loss: 11735423219.1416 - val_loss: 10126207697.0082\n",
      "Epoch 11/150\n",
      "1095/1095 [==============================] - 0s 97us/sample - loss: 7373435177.3808 - val_loss: 8161230169.0740\n",
      "Epoch 12/150\n",
      "1095/1095 [==============================] - 0s 93us/sample - loss: 6528236446.2758 - val_loss: 7341408784.8329\n",
      "Epoch 13/150\n",
      "1095/1095 [==============================] - 0s 119us/sample - loss: 5568904269.6183 - val_loss: 6235383652.2959\n",
      "Epoch 14/150\n",
      "1095/1095 [==============================] - 0s 104us/sample - loss: 4640872015.2548 - val_loss: 5562653543.1014\n",
      "Epoch 15/150\n",
      "1095/1095 [==============================] - 0s 97us/sample - loss: 4157374846.0128 - val_loss: 5048736918.0932\n",
      "Epoch 16/150\n",
      "1095/1095 [==============================] - 0s 105us/sample - loss: 3690457597.8959 - val_loss: 4572435415.3205\n",
      "Epoch 17/150\n",
      "1095/1095 [==============================] - 0s 106us/sample - loss: 3343084659.9598 - val_loss: 4194263599.6932\n",
      "Epoch 18/150\n",
      "1095/1095 [==============================] - 0s 102us/sample - loss: 3070274150.8676 - val_loss: 3876639301.4356\n",
      "Epoch 19/150\n",
      "1095/1095 [==============================] - 0s 104us/sample - loss: 2835433022.4219 - val_loss: 3599637138.5863\n",
      "Epoch 20/150\n",
      "1095/1095 [==============================] - 0s 113us/sample - loss: 2635897724.3763 - val_loss: 3369307286.0932\n",
      "Epoch 21/150\n",
      "1095/1095 [==============================] - 0s 119us/sample - loss: 2471954944.3507 - val_loss: 3164942983.3644\n",
      "Epoch 22/150\n",
      "1095/1095 [==============================] - 0s 114us/sample - loss: 2333636061.3991 - val_loss: 2985934184.5041\n",
      "Epoch 23/150\n",
      "1095/1095 [==============================] - 0s 157us/sample - loss: 2215694772.4858 - val_loss: 2830652942.7288\n",
      "Epoch 24/150\n",
      "1095/1095 [==============================] - 0s 152us/sample - loss: 2110175149.4721 - val_loss: 2696597673.0301\n",
      "Epoch 25/150\n",
      "1095/1095 [==============================] - 0s 136us/sample - loss: 2019751751.0721 - val_loss: 2579535487.2986\n",
      "Epoch 26/150\n",
      "1095/1095 [==============================] - 0s 112us/sample - loss: 1940370464.6137 - val_loss: 2470751551.4740\n",
      "Epoch 27/150\n",
      "1095/1095 [==============================] - 0s 107us/sample - loss: 1870787436.7123 - val_loss: 2379101715.9890\n",
      "Epoch 28/150\n",
      "1095/1095 [==============================] - 0s 112us/sample - loss: 1801715478.0932 - val_loss: 2300743068.0548\n",
      "Epoch 29/150\n",
      "1095/1095 [==============================] - 0s 117us/sample - loss: 1744370938.5059 - val_loss: 2223196969.7315\n",
      "Epoch 30/150\n",
      "1095/1095 [==============================] - 0s 105us/sample - loss: 1692545977.8630 - val_loss: 2159144701.5452\n",
      "Epoch 31/150\n",
      "1095/1095 [==============================] - 0s 102us/sample - loss: 1648764364.9753 - val_loss: 2098466451.6384\n",
      "Epoch 32/150\n",
      "1095/1095 [==============================] - 0s 102us/sample - loss: 1603506470.8091 - val_loss: 2043213196.6247\n",
      "Epoch 33/150\n",
      "1095/1095 [==============================] - 0s 128us/sample - loss: 1560008583.0137 - val_loss: 1994215142.7507\n",
      "Epoch 34/150\n",
      "1095/1095 [==============================] - 0s 107us/sample - loss: 1524800084.7489 - val_loss: 1948785105.0082\n",
      "Epoch 35/150\n",
      "1095/1095 [==============================] - 0s 108us/sample - loss: 1485668596.6027 - val_loss: 1910819339.5726\n",
      "Epoch 36/150\n",
      "1095/1095 [==============================] - 0s 121us/sample - loss: 1452509080.1973 - val_loss: 1872223907.4192\n",
      "Epoch 37/150\n",
      "1095/1095 [==============================] - 0s 103us/sample - loss: 1422232682.0237 - val_loss: 1823687031.2329\n",
      "Epoch 38/150\n",
      "1095/1095 [==============================] - 0s 118us/sample - loss: 1388783805.7205 - val_loss: 1791008703.1233\n",
      "Epoch 39/150\n",
      "1095/1095 [==============================] - 0s 105us/sample - loss: 1359267742.3927 - val_loss: 1758225761.8411\n",
      "Epoch 40/150\n",
      "1095/1095 [==============================] - 0s 104us/sample - loss: 1329049251.3023 - val_loss: 1729699643.9671\n",
      "Epoch 41/150\n",
      "1095/1095 [==============================] - 0s 115us/sample - loss: 1305548436.3397 - val_loss: 1707693732.1205\n",
      "Epoch 42/150\n",
      "1095/1095 [==============================] - 0s 106us/sample - loss: 1277896609.4904 - val_loss: 1675511721.7315\n",
      "Epoch 43/150\n",
      "1095/1095 [==============================] - 0s 100us/sample - loss: 1259256704.8767 - val_loss: 1646562959.0795\n",
      "Epoch 44/150\n",
      "1095/1095 [==============================] - 0s 103us/sample - loss: 1232242522.7105 - val_loss: 1618480929.3151\n",
      "Epoch 45/150\n",
      "1095/1095 [==============================] - 0s 121us/sample - loss: 1211585078.5315 - val_loss: 1595704029.2822\n",
      "Epoch 46/150\n",
      "1095/1095 [==============================] - 0s 115us/sample - loss: 1189755300.5881 - val_loss: 1574565918.5096\n",
      "Epoch 47/150\n",
      "1095/1095 [==============================] - 0s 102us/sample - loss: 1170040162.1918 - val_loss: 1548829172.4274\n",
      "Epoch 48/150\n",
      "1095/1095 [==============================] - 0s 100us/sample - loss: 1149799785.9068 - val_loss: 1534390411.2219\n",
      "Epoch 49/150\n",
      "1095/1095 [==============================] - 0s 99us/sample - loss: 1134833399.1160 - val_loss: 1512475543.4959\n",
      "Epoch 50/150\n",
      "1095/1095 [==============================] - 0s 105us/sample - loss: 1112160661.6840 - val_loss: 1492032803.7699\n",
      "Epoch 51/150\n",
      "1095/1095 [==============================] - 0s 111us/sample - loss: 1096365232.4530 - val_loss: 1469188875.9233\n",
      "Epoch 52/150\n",
      "1095/1095 [==============================] - 0s 107us/sample - loss: 1081895789.9982 - val_loss: 1453115383.4082\n",
      "Epoch 53/150\n",
      "1095/1095 [==============================] - 0s 115us/sample - loss: 1078266480.3361 - val_loss: 1453469503.8247\n",
      "Epoch 54/150\n",
      "1095/1095 [==============================] - 0s 103us/sample - loss: 1050726126.9333 - val_loss: 1423288786.7616\n",
      "Epoch 55/150\n",
      "1095/1095 [==============================] - 0s 131us/sample - loss: 1035601690.7689 - val_loss: 1408153169.8849\n",
      "Epoch 56/150\n",
      "1095/1095 [==============================] - 0s 130us/sample - loss: 1021686341.5525 - val_loss: 1402599857.0959\n",
      "Epoch 57/150\n",
      "1095/1095 [==============================] - 0s 102us/sample - loss: 1014003283.9890 - val_loss: 1374450306.2795\n",
      "Epoch 58/150\n",
      "1095/1095 [==============================] - 0s 107us/sample - loss: 992873888.4968 - val_loss: 1364211778.4548\n",
      "Epoch 59/150\n",
      "1095/1095 [==============================] - 0s 104us/sample - loss: 982952080.8913 - val_loss: 1366861151.0356\n",
      "Epoch 60/150\n",
      "1095/1095 [==============================] - 0s 103us/sample - loss: 965837315.3315 - val_loss: 1343369667.3315\n",
      "Epoch 61/150\n",
      "1095/1095 [==============================] - 0s 100us/sample - loss: 969829001.1763 - val_loss: 1327710699.4849\n",
      "Epoch 62/150\n",
      "1095/1095 [==============================] - 0s 103us/sample - loss: 941838113.5489 - val_loss: 1330517379.8575\n",
      "Epoch 63/150\n",
      "1095/1095 [==============================] - 0s 104us/sample - loss: 936248278.3269 - val_loss: 1312259552.9644\n",
      "Epoch 64/150\n",
      "1095/1095 [==============================] - 0s 99us/sample - loss: 920292583.2767 - val_loss: 1297690783.0356\n",
      "Epoch 65/150\n",
      "1095/1095 [==============================] - 0s 101us/sample - loss: 914939264.1169 - val_loss: 1282791918.4658\n",
      "Epoch 66/150\n",
      "1095/1095 [==============================] - 0s 104us/sample - loss: 900191192.7817 - val_loss: 1282775343.5178\n",
      "Epoch 67/150\n",
      "1095/1095 [==============================] - 0s 105us/sample - loss: 888925002.7836 - val_loss: 1269226001.8849\n",
      "Epoch 68/150\n",
      "1095/1095 [==============================] - 0s 111us/sample - loss: 881599167.8247 - val_loss: 1261577848.6356\n",
      "Epoch 69/150\n",
      "1095/1095 [==============================] - 0s 106us/sample - loss: 871998577.6219 - val_loss: 1252838196.0767\n",
      "Epoch 70/150\n",
      "1095/1095 [==============================] - 0s 101us/sample - loss: 862386946.1041 - val_loss: 1253981420.8877\n",
      "Epoch 71/150\n",
      "1095/1095 [==============================] - 0s 103us/sample - loss: 855936782.0858 - val_loss: 1247824427.3096\n",
      "Epoch 72/150\n",
      "1095/1095 [==============================] - 0s 107us/sample - loss: 843926912.8183 - val_loss: 1236423106.8055\n",
      "Epoch 73/150\n",
      "1095/1095 [==============================] - 0s 107us/sample - loss: 847606556.5808 - val_loss: 1221384983.1452\n",
      "Epoch 74/150\n",
      "1095/1095 [==============================] - 0s 104us/sample - loss: 829507891.2000 - val_loss: 1228863745.4027\n",
      "Epoch 75/150\n",
      "1095/1095 [==============================] - 0s 106us/sample - loss: 822888451.7114 - val_loss: 1221194888.2411\n",
      "Epoch 76/150\n",
      "1095/1095 [==============================] - 0s 120us/sample - loss: 809869101.7936 - val_loss: 1208230474.3452\n",
      "Epoch 77/150\n",
      "1095/1095 [==============================] - 0s 104us/sample - loss: 808239882.3160 - val_loss: 1195812951.6712\n",
      "Epoch 78/150\n",
      "1095/1095 [==============================] - 0s 129us/sample - loss: 794453575.8904 - val_loss: 1203273367.3205\n",
      "Epoch 79/150\n",
      "1095/1095 [==============================] - 0s 116us/sample - loss: 788100262.5753 - val_loss: 1197638075.4411\n",
      "Epoch 80/150\n",
      "1095/1095 [==============================] - 0s 112us/sample - loss: 789869034.0822 - val_loss: 1188862592.3507\n",
      "Epoch 81/150\n",
      "1095/1095 [==============================] - 0s 110us/sample - loss: 779034592.2046 - val_loss: 1198907364.4712\n",
      "Epoch 82/150\n",
      "1095/1095 [==============================] - 0s 116us/sample - loss: 768577286.4584 - val_loss: 1177918312.5041\n",
      "Epoch 83/150\n",
      "1095/1095 [==============================] - 0s 114us/sample - loss: 759360679.8320 - val_loss: 1170875246.2904\n",
      "Epoch 84/150\n",
      "1095/1095 [==============================] - 0s 117us/sample - loss: 756255895.3790 - val_loss: 1169596021.3041\n",
      "Epoch 85/150\n",
      "1095/1095 [==============================] - 0s 111us/sample - loss: 748235774.8311 - val_loss: 1157847106.1041\n",
      "Epoch 86/150\n",
      "1095/1095 [==============================] - 0s 113us/sample - loss: 739655714.8932 - val_loss: 1162854783.6493\n",
      "Epoch 87/150\n",
      "1095/1095 [==============================] - 0s 101us/sample - loss: 735844247.0283 - val_loss: 1154003367.4521\n",
      "Epoch 88/150\n",
      "1095/1095 [==============================] - 0s 121us/sample - loss: 726054041.4247 - val_loss: 1155046613.3918\n",
      "Epoch 89/150\n",
      "1095/1095 [==============================] - 0s 120us/sample - loss: 730088696.0511 - val_loss: 1165184705.5781\n",
      "Epoch 90/150\n",
      "1095/1095 [==============================] - 0s 115us/sample - loss: 727037695.5324 - val_loss: 1140620441.9507\n",
      "Epoch 91/150\n",
      "1095/1095 [==============================] - 0s 121us/sample - loss: 716506676.8073 - val_loss: 1156285385.4685\n",
      "Epoch 92/150\n",
      "1095/1095 [==============================] - 0s 110us/sample - loss: 711654546.2941 - val_loss: 1139030755.9452\n",
      "Epoch 93/150\n",
      "1095/1095 [==============================] - 0s 109us/sample - loss: 699916827.1196 - val_loss: 1140919074.3671\n",
      "Epoch 94/150\n",
      "1095/1095 [==============================] - 0s 114us/sample - loss: 692898418.6740 - val_loss: 1134340412.6685\n",
      "Epoch 95/150\n",
      "1095/1095 [==============================] - 0s 101us/sample - loss: 688983788.1279 - val_loss: 1133428908.3616\n",
      "Epoch 96/150\n",
      "1095/1095 [==============================] - 0s 106us/sample - loss: 681421920.3215 - val_loss: 1132918940.0548\n",
      "Epoch 97/150\n",
      "1095/1095 [==============================] - 0s 102us/sample - loss: 677711189.4210 - val_loss: 1129388882.5863\n",
      "Epoch 98/150\n",
      "1095/1095 [==============================] - 0s 113us/sample - loss: 674755180.0986 - val_loss: 1128640057.1616\n",
      "Epoch 99/150\n",
      "1095/1095 [==============================] - 0s 100us/sample - loss: 671916609.5196 - val_loss: 1116104370.6740\n",
      "Epoch 100/150\n",
      "1095/1095 [==============================] - 0s 100us/sample - loss: 662391221.5379 - val_loss: 1126322629.2603\n",
      "Epoch 101/150\n",
      "1095/1095 [==============================] - 0s 100us/sample - loss: 660791186.4694 - val_loss: 1118432054.7068\n",
      "Epoch 102/150\n",
      "1095/1095 [==============================] - 0s 115us/sample - loss: 653780994.1333 - val_loss: 1113266055.1890\n",
      "Epoch 103/150\n",
      "1095/1095 [==============================] - 0s 69us/sample - loss: 648981171.9014 - val_loss: 1112485392.3068\n",
      "Epoch 104/150\n",
      "1095/1095 [==============================] - 0s 100us/sample - loss: 641009846.9406 - val_loss: 1131086903.5836\n",
      "Epoch 105/150\n",
      "1095/1095 [==============================] - 0s 165us/sample - loss: 641928413.5160 - val_loss: 1123069674.2575\n",
      "Epoch 106/150\n",
      "1095/1095 [==============================] - 0s 127us/sample - loss: 641439450.3014 - val_loss: 1104177099.9233\n",
      "Epoch 107/150\n",
      "1095/1095 [==============================] - 0s 104us/sample - loss: 627723154.1626 - val_loss: 1117439280.2192\n",
      "Epoch 108/150\n",
      "1095/1095 [==============================] - 0s 121us/sample - loss: 628260019.5215 - val_loss: 1113064004.3836\n",
      "Epoch 109/150\n",
      "1095/1095 [==============================] - 0s 103us/sample - loss: 620777181.7498 - val_loss: 1105237384.0658\n",
      "Epoch 110/150\n",
      "1095/1095 [==============================] - 0s 98us/sample - loss: 625048362.1991 - val_loss: 1098338157.9397\n",
      "Epoch 111/150\n",
      "1095/1095 [==============================] - 0s 110us/sample - loss: 615174240.4384 - val_loss: 1101603981.5014\n",
      "Epoch 112/150\n",
      "1095/1095 [==============================] - 0s 104us/sample - loss: 609594591.7954 - val_loss: 1093234359.0575\n",
      "Epoch 113/150\n",
      "1095/1095 [==============================] - 0s 108us/sample - loss: 606149323.3388 - val_loss: 1094249833.5562\n",
      "Epoch 114/150\n",
      "1095/1095 [==============================] - 0s 108us/sample - loss: 601709461.6256 - val_loss: 1099269324.9753\n",
      "Epoch 115/150\n",
      "1095/1095 [==============================] - 0s 104us/sample - loss: 599384380.7269 - val_loss: 1103170428.4932\n",
      "Epoch 116/150\n",
      "1095/1095 [==============================] - 0s 127us/sample - loss: 595131881.7607 - val_loss: 1097555543.6712\n",
      "Epoch 117/150\n",
      "1095/1095 [==============================] - 0s 104us/sample - loss: 590432966.6630 - val_loss: 1091915840.3507\n",
      "Epoch 118/150\n",
      "1095/1095 [==============================] - 0s 108us/sample - loss: 588150165.2164 - val_loss: 1105649706.4329\n",
      "Epoch 119/150\n",
      "1095/1095 [==============================] - 0s 107us/sample - loss: 584003689.4685 - val_loss: 1088087338.4329\n",
      "Epoch 120/150\n",
      "1095/1095 [==============================] - 0s 101us/sample - loss: 580193874.5279 - val_loss: 1088048667.7041\n",
      "Epoch 121/150\n",
      "1095/1095 [==============================] - 0s 101us/sample - loss: 578970469.5817 - val_loss: 1089418232.8110\n",
      "Epoch 122/150\n",
      "1095/1095 [==============================] - 0s 98us/sample - loss: 571152506.9297 - val_loss: 1086393653.6548\n",
      "Epoch 123/150\n",
      "1095/1095 [==============================] - 0s 105us/sample - loss: 568251725.3260 - val_loss: 1089284231.5397\n",
      "Epoch 124/150\n",
      "1095/1095 [==============================] - 0s 110us/sample - loss: 563208850.1041 - val_loss: 1094629249.9288\n",
      "Epoch 125/150\n",
      "1095/1095 [==============================] - 0s 107us/sample - loss: 563240216.2557 - val_loss: 1087413011.1123\n",
      "Epoch 126/150\n",
      "1095/1095 [==============================] - 0s 103us/sample - loss: 556637079.4959 - val_loss: 1079627849.4685\n",
      "Epoch 127/150\n",
      "1095/1095 [==============================] - 0s 106us/sample - loss: 553773782.4438 - val_loss: 1090814406.8384\n",
      "Epoch 128/150\n",
      "1095/1095 [==============================] - 0s 115us/sample - loss: 558631709.1653 - val_loss: 1094692625.3589\n",
      "Epoch 129/150\n",
      "1095/1095 [==============================] - 0s 112us/sample - loss: 556508443.3242 - val_loss: 1069889747.8137\n",
      "Epoch 130/150\n",
      "1095/1095 [==============================] - 0s 109us/sample - loss: 552853816.4018 - val_loss: 1105572084.0767\n",
      "Epoch 131/150\n",
      "1095/1095 [==============================] - 0s 98us/sample - loss: 543260322.3671 - val_loss: 1077673462.5315\n",
      "Epoch 132/150\n",
      "1095/1095 [==============================] - 0s 107us/sample - loss: 541131769.2201 - val_loss: 1090005967.6055\n",
      "Epoch 133/150\n",
      "1095/1095 [==============================] - 0s 100us/sample - loss: 536228848.6868 - val_loss: 1083460810.6959\n",
      "Epoch 134/150\n",
      "1095/1095 [==============================] - 0s 95us/sample - loss: 531881780.6174 - val_loss: 1072764016.7452\n",
      "Epoch 135/150\n",
      "1095/1095 [==============================] - 0s 98us/sample - loss: 529134965.7717 - val_loss: 1088469275.5288\n",
      "Epoch 136/150\n",
      "1095/1095 [==============================] - 0s 106us/sample - loss: 524605726.0566 - val_loss: 1077287442.0603\n",
      "Epoch 137/150\n",
      "1095/1095 [==============================] - 0s 114us/sample - loss: 522616543.3863 - val_loss: 1074308390.9260\n",
      "Epoch 138/150\n",
      "1095/1095 [==============================] - 0s 115us/sample - loss: 520082561.8411 - val_loss: 1078322916.8219\n",
      "Epoch 139/150\n",
      "1095/1095 [==============================] - 0s 104us/sample - loss: 515481581.7644 - val_loss: 1075494553.7753\n",
      "Epoch 140/150\n",
      "1095/1095 [==============================] - 0s 116us/sample - loss: 515051049.6146 - val_loss: 1079719371.0466\n",
      "Epoch 141/150\n",
      "1095/1095 [==============================] - 0s 106us/sample - loss: 510184401.3589 - val_loss: 1074981416.8548\n",
      "Epoch 142/150\n",
      "1095/1095 [==============================] - 0s 107us/sample - loss: 507256805.8155 - val_loss: 1077619538.7616\n",
      "Epoch 143/150\n",
      "1095/1095 [==============================] - 0s 100us/sample - loss: 505848473.3370 - val_loss: 1072789131.0466\n",
      "Epoch 144/150\n",
      "1095/1095 [==============================] - 0s 102us/sample - loss: 500748895.5324 - val_loss: 1083618363.2658\n",
      "Epoch 145/150\n",
      "1095/1095 [==============================] - 0s 115us/sample - loss: 498763383.2913 - val_loss: 1077263347.5507\n",
      "Epoch 146/150\n",
      "1095/1095 [==============================] - 0s 103us/sample - loss: 496915404.8000 - val_loss: 1079706561.4027\n",
      "Epoch 147/150\n",
      "1095/1095 [==============================] - 0s 111us/sample - loss: 493863850.8128 - val_loss: 1080648738.3671\n",
      "Epoch 148/150\n",
      "1095/1095 [==============================] - 0s 106us/sample - loss: 491578472.6502 - val_loss: 1089791156.2521\n",
      "Epoch 149/150\n",
      "1095/1095 [==============================] - 0s 107us/sample - loss: 490562684.8146 - val_loss: 1078305512.1534\n",
      "Epoch 150/150\n",
      "1095/1095 [==============================] - 0s 107us/sample - loss: 486645662.3635 - val_loss: 1090441076.6027\n",
      "RMSE: 0.1817\n",
      "モデルの作成終了: 2020-02-14 06:28:21.796167\n"
     ]
    }
   ],
   "source": [
    "printTime('モデルの作成開始')\n",
    "\n",
    "# 学習データを学習データとバリデーションデータに分ける\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=71)\n",
    "tr_idx, va_idx = list(kf.split(train_x))[0]\n",
    "tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "\n",
    "\n",
    "# tensorflowの警告抑制\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# データのスケーリング\n",
    "scaler = StandardScaler()\n",
    "tr_x = scaler.fit_transform(tr_x)\n",
    "va_x = scaler.transform(va_x)\n",
    "test_x = scaler.transform(test_x)\n",
    "\n",
    "# pandas.core.series.Series型のままだと学習時にエラーするので、ndarrayに変換\n",
    "tr_y = np.asarray(tr_y)\n",
    "va_y = np.asarray(va_y)\n",
    "\n",
    "#ニューラルネットワークモデルの生成\n",
    "#モデルの構成は\n",
    "#入力(13次元)-1000次元-800次元-100次元-予測値(1次元)\n",
    "#Sequential()というのは入力から枝分かれせずに書けるモデルのクラス\n",
    "#Dense()は全結合層\n",
    "#activation は活性化関数，ここではReLUを用いている\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1000, activation = 'relu'))\n",
    "model.add(Dense(800, activation = 'relu'))\n",
    "model.add(Dense(100, activation =  'relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# モデルをコンパイル\n",
    "# コンパイルすることで逆伝搬などが準備される\n",
    "# 最適化関数として Adam を指定\n",
    "# 誤差関数には平均2乗誤差を指定\n",
    " \n",
    "\n",
    "model.compile(Adam(lr=1e-3), loss=\"mean_squared_error\")\n",
    "\n",
    "\n",
    "#トレーニングデータで学習し，テストデータで評価\n",
    "\n",
    "# batch_size: ミニバッチのデータ数\n",
    "# epochs: 全部のデータについて処理する回数 1epoch = 1周\n",
    "# verbose: 表示の形式，0だと何も表示されない\n",
    "# validation_data: 評価用データ（ここではハイパーパラメータの調整をしないので\n",
    "#   テストデータをそのまま用いている）\n",
    "\n",
    "history = model.fit(tr_x, \n",
    "                    tr_y, \n",
    "                    batch_size=128, \n",
    "                    epochs=150, \n",
    "                    verbose=1, \n",
    "                    validation_data=(va_x, va_y))\n",
    "\n",
    "# バリデーションデータでのスコアの確認\n",
    "va_pred = model.predict(va_x)\n",
    "\n",
    "# バリデーションデータでのスコア(真の値の対数と予測値の対数の二乗平均平方根誤差 (RMSE))を計算する\n",
    "rmse = np.sqrt(mean_squared_error(np.log(va_y), np.log(va_pred)))\n",
    "\n",
    "print(f'RMSE: {rmse:.4f}')\n",
    "\n",
    "# 予測\n",
    "pred = model.predict(test_x)\n",
    "\n",
    "pred = pd.DataFrame(pred)[0]\n",
    "\n",
    "# 提出用ファイルの作成\n",
    "submission = pd.DataFrame({'Id': test['Id'], 'SalePrice': pred})\n",
    "submission.to_csv('/kaggle/output/submission_first.csv', index=False)\n",
    "\n",
    "printTime('モデルの作成終了')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### モデルチューニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデルチューニング開始: 2020-02-14 06:23:05.290351\n",
      "モデルチューニング終了: 2020-02-14 06:23:05.291321\n"
     ]
    }
   ],
   "source": [
    "printTime('モデルチューニング開始')\n",
    "\n",
    "\"\"\"\n",
    "# チューニング候補とするパラメータを準備する\n",
    "param_space = {\n",
    "    'max_depth': [4, 5, 6],\n",
    "    'min_child_weight': [0.3, 0.4, 0.5]\n",
    "}\n",
    "\n",
    "# 探索するハイパーパラメータの組み合わせ\n",
    "param_combinations = itertools.product(param_space['max_depth'], param_space['min_child_weight'])\n",
    "\n",
    "# 各パラメータの組み合わせ、それに対するスコアを保存するリスト\n",
    "params = []\n",
    "scores = []\n",
    "\n",
    "# ループカウント変数\n",
    "loopcount = 0\n",
    "\n",
    "# 各パラメータの組み合わせごとに、クロスバリデーションで評価を行う\n",
    "for max_depth, min_child_weight in param_combinations:\n",
    "\n",
    "    loopcount += 1\n",
    "    \n",
    "    score_folds = []\n",
    "    # クロスバリデーションを行う\n",
    "    # 学習データを4つに分割し、うち1つをバリデーションデータとすることを、バリデーションデータを変えて繰り返す\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=123456)\n",
    "    for tr_idx, va_idx in kf.split(train_x):\n",
    "        # 学習データを学習データとバリデーションデータに分ける\n",
    "        tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "        tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "\n",
    "        # モデルの学習を行う\n",
    "        model = XGBClassifier(n_estimators=20, random_state=71,\n",
    "                              max_depth=max_depth, min_child_weight=min_child_weight)\n",
    "        model.fit(tr_x, tr_y)\n",
    "\n",
    "        # バリデーションデータでのスコアを計算し、保存する\n",
    "        va_pred = model.predict(va_x)\n",
    "        rmse = np.sqrt(mean_squared_error(np.log(va_y), np.log(va_pred)))\n",
    "        score_folds.append(rmse)\n",
    "\n",
    "    # 各foldのスコアを平均する\n",
    "    score_mean = np.mean(score_folds)\n",
    "\n",
    "    # パラメータの組み合わせ、それに対するスコアを保存する\n",
    "    params.append((max_depth, min_child_weight))\n",
    "    scores.append(score_mean)\n",
    "\n",
    "    printTime(f'max_depth: {max_depth}, '\\\n",
    "              f'min_child_weight: {min_child_weight}, '\\\n",
    "              f'score: {score_mean:.5f}')\n",
    "    \n",
    "# 最もスコアが良いものをベストなパラメータとする\n",
    "best_idx = np.argsort(scores)[0]\n",
    "best_param = params[best_idx]\n",
    "print(f'max_depth: {best_param[0]}, min_child_weight: {best_param[1]}')\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "printTime('モデルチューニング終了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
